{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soynlp -q\n",
    "\n",
    "#Preprocess: text를 전처리하는 함수\n",
    "\n",
    "import re\n",
    "import json\n",
    "from soynlp.normalizer import *\n",
    "\n",
    "def PreProcess(text):\n",
    "    bad_chars = {\"\\u200b\": \"\", \"…\": \" ... \", \"\\ufeff\": \"\"}\n",
    "    for bad_char in bad_chars:\n",
    "        text = text.replace(bad_char, bad_chars[bad_char]) # 문제를 일으킬 수 있는 문자 제거(너비가 0인 공백문자,인코딩 정보를 알려주는 역할 문자)\n",
    "    error_chars = {\"\\u3000\": \" \", \"\\u2009\": \" \", \"\\u2002\": \" \", \"\\xa0\":\" \"} #(표의 공백문자, 한칸, 엔터+스페이스, 빈 공백-유니코드에서 )\n",
    "    for error_char in error_chars:\n",
    "        text = text.replace(error_char, error_chars[error_char])\n",
    "    # text = re.sub(pattern='Posted on [0-9]{4} [0-9]{2} [0-9]{2} .+ Posted in \\S+ \\s?', \\\n",
    "    #               repl='', string=text) #정규 표현식 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체 (년도)\n",
    "    # text = re.sub(pattern='Posted on [0-9]{8} .+ Posted in \\S+ \\s?', \\\n",
    "    #               repl='', string=text)\n",
    "    # text = re.sub(pattern='[0-9]{4}년 [0-9]{,2}월 [0-9]{,2}일 [0-9]{,2}시 [0-9]{,2}분 [0-9]{,2}초', \\\n",
    "    #               repl='', string=text) #(년도 시간)\n",
    "    # text = re.sub(pattern='[0-9]{4}. [0-9]{,2}. [0-9]{,2}', \\\n",
    "    #               repl='', string=text) #필요없는 부분 제거 (년도 제거)\n",
    "    text = re.sub(pattern = '^[a-zA-Z0-9]+@[a-zA-Z0-9]+$', \\\n",
    "                  repl ='', string = text) #이메일 제거\n",
    "    text = re.sub(pattern = '#\\S+', \\\n",
    "                  repl ='', string = text) # \"#문자\" 형식 어절 제거\n",
    "    text = re.sub(pattern = '@\\w+', \\\n",
    "                  repl ='', string = text) # \"@문자\" 형식 어절 제거\n",
    "    text = re.sub(pattern = '\\(출처 ?= ?.+\\) |\\(사진 ?= ?.+\\) |\\(자료 ?= ?.+\\)| \\(자료사진\\) |사진=.+기자 ', \\\n",
    "                  repl ='', string = text) # 뉴스 내 포함된 이미지에 대한 레이블 제거\n",
    "    text = repeat_normalize(text, num_repeats=2).strip() # 중복 문자 처리\n",
    "    _filter = re.compile('https?://\\S+|www\\.\\S+')\n",
    "    text = _filter.sub('', text) #url 전처리\n",
    "    _filter = re.compile('[ㄱ-ㅣ]+') \n",
    "    text = _filter.sub('', text) #자모음제거\n",
    "    _filter = re.compile('.co\\S')\n",
    "    text = _filter.sub('', text) #.co 전처리\n",
    "    _filter = re.compile('[一-龥]')\n",
    "    text = _filter.sub('', text) #한자 전처리  \n",
    "    _filter = re.compile('[^가-힣 0-9 a-z A-Z \\. \\, \\' \\\" \\? \\!]+') \n",
    "    text = _filter.sub('', text) #(\".\",\",\",\"'\",\"?\",\"!\")를 제외한 특수문자 제거\n",
    "    # _filter = re.compile('[\\n]')\n",
    "    # text = _filter.sub(\"\", text) #개행제거\n",
    "    return text\n",
    "\n",
    "\n",
    "#save_data: 데이터를 합쳐서 저장하는 함수\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "def save_data(file_path, save_path):\n",
    "    print('{} data saving.'.format(file_path.split('/')[-1]))\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        text = json_data['data']\n",
    "        length = len(text)\n",
    "        for i in range (length):\n",
    "            text[i]['text'] = PreProcess(text[i][\"text\"])\n",
    "    data={}\n",
    "    data[\"data\"] = text\n",
    "\n",
    "    with open(save_path_1, 'w') as outfile:\n",
    "        json.dump(data, outfile, ensure_ascii=False)    \n",
    "      \n",
    "\n",
    "\n",
    "    print(\"\\nAll saved.\".format(file_path.split('/')[-1]))\n",
    "\n",
    "\n",
    "#합칠 데이터와 새로 저장할 데이터 Path 지정 => 전처리할 데이터와 전처리 저장할 데이터에 맞는 위치에 맞게 변경\n",
    "\n",
    "file_1 = './글틴2.json' #크롤링 후 수집한 데이터의 위치(전처리 이전) => 팀 데이터 보관위치에 맞게 변경\n",
    "save_path_1 = './data_글틴.json' #저장할 데이터 위치(전처리된) => 팀 데이터 보관할 위치에 맞게 변경\n",
    "\n",
    "#함수 실행\n",
    "\n",
    "save_data(file_1, save_path_1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
